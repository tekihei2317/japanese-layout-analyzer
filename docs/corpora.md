# コーパスについて

コーパスの形式をどうするべきか考えています。かなn-gramか、モーラn-gramか、単語n-gramか。

指標は打鍵2-gramと3-gramに対するものなので、それらを正確に計算できるようにする必要があります。

また、どの打鍵を優先するかをカスタマイズできるようにもしたいです。

例えば、

- 同手、クロスシフトの両方が使える配列で、どちらを使うのか場合に応じて柔軟に変えたい
- 大西配列で、syaのsyは同指連続なのでshaを使うようにしたい

みたいなケースがあります。

## 正確に計算するには

例えば「しゃ」を「し」「ゃ」に区切ってしまうと、ローマ字入力や、モーラ単位で入力する配列のデータを正しく計算できません。

他にも、ブリ中トロ配列には「です」「ます」などの入力方法があります。

有名なのはkouyさんの100万字データですが、これはかな単位なのでそのまま使うのは難しい。他に公開されているn-gramデータには、月見草の1800万字データ、小梅配列の10万字データなどがあります。

小梅配列はまだ探していないのですが、月見草もかな単位のデータでした。

---

なので、ひとまずはひらがなの文章に対する打鍵数を計算して、それに近い値を出せるようにモーラ単位のコーパスを作ればいいのかなと思います。

```text
# テキストファイルと配列を指定して分析
jla analyze <file> <layout> [--format <text|json>]

# コーパスと配列を指定して分析
jla analyze <corpus> <layout> [--format <text|json>]
```

### テキストファイルを使用した分析

幅優先探索で最短となるようなストロークを見つけてから、各指標を計算します。

### コーパスを使用した分析

最初にモーラ単位でストロークを作成しておいて連結して計算したら良さそうです。

問題がいくつかあって、例えば2-gramのデータについて

```text
100 う ん
50 ん あ
```

みたいなデータがあったとします。ローマ字で「ん→n」というルールを作っておくのですが、これだと「んの後に母音またはナ行が来るケース」を少なくカウントしてしまいます。

```text
あ a
う u
ん n
んあ nna
んい nni
んう nnu
んえ nne
んお nno
んな nnna
んに nnni
んぬ nnnu
んね nnne
んの nnno
```

みたいなファイルを作って、ルールを上書きして別でカウントする、みたいな感じでできるかな。「んあ」を「n + na」と考えて、nnとnaの連節をカウントしてやる必要があります。

あとはローマ字の場合は促音にも気をつけないといけない。

```text
100 か っ
50 っ た
```

みたいなデータがある時。基本的に「っ」は1打鍵と打鍵数的には考えればいいんだけど、後ろに何が来ているか分からないと何を打ったのかが分からない。

なのでこの場合は、「っ」自体は0打鍵として...はダメか。kattaのatが計算されないから。これ難しい...。

ちゃんと数えようとすると「っ+次の文字」を1つの単位としてコーパスを作るしかないか。それか3-gramのデータ参照して次に来る文字の割合を確認するとか...。

```text
っか
っき
っく
っけ
っこ
っさ
っし
っす
っせ
っそ
った
っち
っつ
って
っと
っぱ
っぴ
っぷ
```

## ひらがなの文章に対する打鍵数の計算について

- デフォルトではローマ字テーブルを活用し
- カスタマイズするための仕組みも用意する

みたいな感じにできるのか考えてみましょう。

ローマ字テーブルを使う方法ですが、ストローク→かなの変換ロジックはtomoemonさんのを参考にAIに丸投げして未理解なので、まずはそこからです。
